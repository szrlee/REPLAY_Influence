# Influence Analysis Project (MAGIC & LDS)

## Overview

This project implements and explores methods for understanding how individual training data points affect a model's predictions and internal states. It focuses on:

1.  **Influence Function Calculation (MAGIC Analysis):** This involves computing influence scores for training samples, similar to methods like TracIn/REPLAY. These scores help identify which training examples were most responsible for a model's behavior on a specific test (or validation) instance. This is invaluable for model debugging, understanding dataset biases, and identifying influential or potentially mislabeled training data.
2.  **Validation of Influence Scores (LDS Validation):** A method to empirically validate the computed influence scores by training multiple models on systematically generated subsets of the training data. The performance of these subset-trained models on a target instance is then correlated with the influence scores, providing a sanity check for the influence estimation.

The project uses the CIFAR-10 dataset and a ResNet9 model architecture as a concrete testbed for these analyses.

## Project Structure

The project is organized as follows:

```
influence_project/
├── src/                    # Source code for the analyses
│   ├── __init__.py         # Package initializer
│   ├── config.py           # Shared configurations, hyperparameters, and paths
│   ├── data_handling.py    # CIFAR-10 dataset loading and preprocessing logic
│   ├── model_def.py        # ResNet9 model architecture definition
│   ├── magic_analyzer.py   # Core implementation of MAGIC influence calculation (TracIn/REPLAY-like)
│   ├── lds_validator.py    # Core implementation of LDS subset training and validation
│   └── visualization.py    # Plotting functions for visualizing influence and correlation
├── main_runner.py          # Main script to execute MAGIC and/or LDS analyses
├── outputs/                # Directory for all generated files
│   ├── checkpoints_magic/  # Model checkpoints saved during MAGIC model training (one per step)
│   ├── checkpoints_lds/    # Final model checkpoints for each LDS subset model
│   ├── scores_magic/       # Pickled NumPy arrays of computed MAGIC influence scores
│   ├── losses_lds/         # Pickled NumPy arrays of per-sample validation losses for each LDS model
│   └── plots/              # Generated plots (MAGIC influential images, LDS correlation plots)
├── data/                   # (Currently unused, CIFAR-10 downloads to /tmp/cifar/ by default via config.py)
├── notebooks/              # (Optional, for experimental code and exploration)
├── README.md               # This file
└── requirements.txt        # Python dependencies
```

## Requirements

*   Python 3.8+
*   PyTorch (see `requirements.txt` for version, e.g., 1.10+)
*   NumPy
*   Matplotlib
*   Seaborn
*   tqdm
*   SciPy

For a full list of dependencies and their versions, please refer to `requirements.txt`.

## Setup

1.  **Create a virtual environment (recommended):**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Linux/macOS
    # venv\Scripts\activate    # On Windows
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Running the Analyses

The primary entry point for running the analyses is `main_runner.py`. You can see all available options by running:
```bash
python main_runner.py --help
```

**Workflow:**

The typical workflow involves:
1.  Running the MAGIC analysis to compute influence scores for a target validation image.
2.  Optionally, running the LDS validation, which uses the scores from the MAGIC analysis.

**Examples:**

1.  **Run only the MAGIC influence analysis:**
    ```bash
    python main_runner.py --run_magic
    ```
    This will:
    *   Train a ResNet9 model on CIFAR-10 from scratch, saving model checkpoints at each training step to `outputs/checkpoints_magic/`.
    *   Compute influence scores for a pre-defined target validation image (configured in `src/config.py`).
    *   Save the raw influence scores as a pickled NumPy array to `outputs/scores_magic/`.
    *   Generate and save a plot of the most and least influential training images for the target image to `outputs/plots/`.

2.  **Run only the LDS validation (requires pre-computed MAGIC scores):**
    LDS validation relies on influence scores generated by the MAGIC analysis.
    *   If `magic_analyzer.py` was run previously and its `MAGIC_TARGET_VAL_IMAGE_IDX` matches `LDS_TARGET_VAL_IMAGE_IDX_FOR_CORRELATION` in `src/config.py`, LDS validation can find the scores file automatically.
      ```bash
      python main_runner.py --run_lds
      ```
    *   Alternatively, you can explicitly specify the path to the MAGIC scores file:
      ```bash
      python main_runner.py --run_lds --magic_scores_file outputs/scores_magic/magic_scores_val_21.pkl
      ```
    This will:
    *   Generate definitions for training data subsets (or load them if they exist from a previous run, see `outputs/indices_lds.pkl`).
    *   Train multiple ResNet9 models, each on a different data subset, saving their final checkpoints to `outputs/checkpoints_lds/`.
    *   Evaluate these subset-trained models on the entire validation set and save their per-sample validation losses to `outputs/losses_lds/`.
    *   Correlate the performance of these subset-trained models (specifically, their loss on the target validation image) with predictions derived from the MAGIC influence scores.
    *   Generate and save a correlation plot to `outputs/plots/`.

3.  **Run both MAGIC and LDS sequentially:**
    ```bash
    python main_runner.py --run_magic --run_lds
    ```
    This is a convenient way to ensure the MAGIC scores are generated and then immediately used by the LDS validation, especially if the target validation image for both analyses is the same (as per default configuration).

## Configuration

Key parameters, paths, and algorithm settings can be modified in `src/config.py`. This includes:

*   Target validation image indices for MAGIC and LDS analyses.
*   Hyperparameters for model training (e.g., number of epochs, batch sizes, learning rates) for both MAGIC model and LDS subset models.
*   Paths for saving outputs (checkpoints, scores, plots).
*   Parameters for the LDS validation (e.g., number of subset models to train, fraction of data per subset).
*   The learning rate used in the REPLAY influence calculation (`MAGIC_REPLAY_LEARNING_RATE`).

## Notes

*   **CIFAR-10 Data Path:** The scripts assume CIFAR-10 will be downloaded to `/tmp/cifar/` by default. This can be changed by modifying `CIFAR_ROOT` in `src/config.py`.
*   **Memory Usage (MAGIC Analysis):** The `batch_dict_for_replay` variable in `src/magic_analyzer.py` stores all training batches (images, labels, indices) in CPU memory during the REPLAY computation. For larger datasets or significantly longer training runs, this could lead to high memory consumption. This is a known trade-off for this variant of influence calculation. Potential optimizations for larger scales (not implemented here) could involve saving batch data to disk and reloading as needed.
*   **Relative Imports:** The project uses relative imports within the `src` package (e.g., `from .config import ...`). This means scripts within `src` might need to be run as part of the package (e.g., using `python -m src.magic_analyzer`) if run directly, though the primary entry point is `main_runner.py` from the project root.